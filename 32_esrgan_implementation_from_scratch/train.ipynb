{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbcbe61a-9504-4e57-8587-8518fc13f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import config\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from utils import gradient_penalty, load_checkpoint, save_checkpoint, plot_examples\n",
    "from loss import VGGLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from model import Generator, Discriminator, initialize_weights\n",
    "from tqdm import tqdm\n",
    "from dataset import MyImageFolder\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b540911-0bae-4b7b-b8c3-da0310266cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4172f98d-c452-43cd-99db-124e5f85aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    loader,\n",
    "    disc,\n",
    "    gen,\n",
    "    opt_gen,\n",
    "    opt_disc,\n",
    "    l1,\n",
    "    vgg_loss,\n",
    "    g_scaler,\n",
    "    d_scaler,\n",
    "    writer,\n",
    "    tb_step,\n",
    "):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for idx, (low_res, high_res) in enumerate(loop):\n",
    "        high_res = high_res.to(config.DEVICE)\n",
    "        low_res = low_res.to(config.DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen(low_res)\n",
    "            critic_real = disc(high_res)\n",
    "            critic_fake = disc(fake.detach())\n",
    "            gp = gradient_penalty(disc, high_res, fake, device=config.DEVICE)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "                + config.LAMBDA_GP * gp\n",
    "            )\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        d_scaler.scale(loss_critic).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update()\n",
    "\n",
    "        # Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        with torch.cuda.amp.autocast():\n",
    "            l1_loss = 1e-2 * l1(fake, high_res)\n",
    "            adversarial_loss = 5e-3 * -torch.mean(disc(fake))\n",
    "            loss_for_vgg = vgg_loss(fake, high_res)\n",
    "            gen_loss = l1_loss + loss_for_vgg + adversarial_loss\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(gen_loss).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update()\n",
    "\n",
    "        writer.add_scalar(\"Critic loss\", loss_critic.item(), global_step=tb_step)\n",
    "        tb_step += 1\n",
    "\n",
    "        if idx % 100 == 0 and idx > 0:\n",
    "            plot_examples(\"test_images/\", gen)\n",
    "\n",
    "        loop.set_postfix(\n",
    "            gp=gp.item(),\n",
    "            critic=loss_critic.item(),\n",
    "            l1=l1_loss.item(),\n",
    "            vgg=loss_for_vgg.item(),\n",
    "            adversarial=adversarial_loss.item(),\n",
    "        )\n",
    "\n",
    "    return tb_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd252651-e24a-409b-9895-324e76f55619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset = MyImageFolder(root_dir=\"data/\")\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "    )\n",
    "    gen = Generator(in_channels=3).to(config.DEVICE)\n",
    "    disc = Discriminator(in_channels=3).to(config.DEVICE)\n",
    "    initialize_weights(gen)\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.9))\n",
    "    opt_disc = optim.Adam(disc.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.9))\n",
    "    writer = SummaryWriter(\"logs\")\n",
    "    tb_step = 0\n",
    "    l1 = nn.L1Loss()\n",
    "    gen.train()\n",
    "    disc.train()\n",
    "    vgg_loss = VGGLoss()\n",
    "\n",
    "    g_scaler = torch.cuda.amp.GradScaler()\n",
    "    d_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    if config.LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_GEN,\n",
    "            gen,\n",
    "            opt_gen,\n",
    "            config.LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_DISC,\n",
    "            disc,\n",
    "            opt_disc,\n",
    "            config.LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        tb_step = train_fn(\n",
    "            loader,\n",
    "            disc,\n",
    "            gen,\n",
    "            opt_gen,\n",
    "            opt_disc,\n",
    "            l1,\n",
    "            vgg_loss,\n",
    "            g_scaler,\n",
    "            d_scaler,\n",
    "            writer,\n",
    "            tb_step,\n",
    "        )\n",
    "\n",
    "        if config.SAVE_MODEL:\n",
    "            save_checkpoint(gen, opt_gen, filename=config.CHECKPOINT_GEN)\n",
    "            save_checkpoint(disc, opt_disc, filename=config.CHECKPOINT_DISC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188d4f5a-b9bd-41de-b3e3-5c39cdd66b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gen.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m     gen = Generator(in_channels=\u001b[32m3\u001b[39m).to(config.DEVICE)\n\u001b[32m      8\u001b[39m     opt_gen = optim.Adam(gen.parameters(), lr=config.LEARNING_RATE, betas=(\u001b[32m0.0\u001b[39m, \u001b[32m0.9\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCHECKPOINT_GEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopt_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     plot_examples(\u001b[33m\"\u001b[39m\u001b[33mtest_images/\u001b[39m\u001b[33m\"\u001b[39m, gen)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# This will train from scratch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\pro\\pytorch-neural-network\\32_esrgan_implementation_from_scratch\\utils.py:43\u001b[39m, in \u001b[36mload_checkpoint\u001b[39m\u001b[34m(checkpoint_file, model, optimizer, lr)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_checkpoint\u001b[39m(checkpoint_file, model, optimizer, lr):\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=> Loading checkpoint\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# model.load_state_dict(checkpoint)\u001b[39;00m\n\u001b[32m     45\u001b[39m     model.load_state_dict(checkpoint[\u001b[33m\"\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'gen.pth'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try_model = True\n",
    "\n",
    "    if try_model:\n",
    "        # Will just use pretrained weights and run on images\n",
    "        # in test_images/ and save the ones to SR in saved/\n",
    "        gen = Generator(in_channels=3).to(config.DEVICE)\n",
    "        opt_gen = optim.Adam(gen.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.9))\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_GEN,\n",
    "            gen,\n",
    "            opt_gen,\n",
    "            config.LEARNING_RATE,\n",
    "        )\n",
    "        plot_examples(\"test_images/\", gen)\n",
    "    else:\n",
    "        # This will train from scratch\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
